{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# tools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob\n",
    "from arglex.Classifier import Classifier\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "arglex = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/users.json\", \"r\") as f:\n",
    "    users = json.load(f)\n",
    "\n",
    "with open(\"./data/debates.json\", \"r\") as f:\n",
    "    debates = json.load(f)\n",
    "    \n",
    "with open(\"big_issue_embedding.json\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    issue_emb_dic = json.load(f)\n",
    "\n",
    "with open(\"user_attritbute_embedding.json\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    att_emb_dic = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the debate dataset into a desired form for the model\n",
    "user_collection = {}\n",
    "cats = list(users[\"ahuggies30\"].keys())\n",
    "useful_cats= ['political_ideology', 'education', 'ethnicity', 'interested', 'gender' , 'religious_ideology']\n",
    "\n",
    "per_cat_choices = dict()\n",
    "for cat in useful_cats:\n",
    "    per_cat_choices[cat] = []\n",
    "\n",
    "for name,user in users.items():\n",
    "    user_data = dict()\n",
    "    user_data[\"name\"] = (name)\n",
    "    \n",
    "    # categorical data\n",
    "    for cat in useful_cats:\n",
    "        user_data[cat] = user[cat]\n",
    "\n",
    "        if user[cat] not in per_cat_choices[cat]:\n",
    "            per_cat_choices[cat].append(user[cat])\n",
    "    \n",
    "    if not (len(debates) == 0 and len(opinions) == 0):\n",
    "        user_collection[name] = user\n",
    "    \n",
    "# prepare one-hot\n",
    "for key, user_data in user_collection.items():\n",
    "    for cat in useful_cats:\n",
    "        user_data[\"name\"] = key\n",
    "        user_data[cat + \"_id\"] = per_cat_choices[cat].index(user_data[cat])\n",
    "        user_data[cat + \"_len\"] = len(per_cat_choices[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['all_debates', 'big_issues_dict', 'birthday', 'description', 'education', 'elo_ranking', 'email', 'ethnicity', 'gender', 'friends', 'income', 'interested', 'joined', 'last_online', 'last_updated', 'looking', 'lost_debates', 'number_of_all_debates', 'number_of_lost_debates', 'number_of_tied_debates', 'number_of_won_debates', 'number_of_friends', 'number_of_opinion_arguments', 'number_of_opinion_questions', 'number_of_poll_topics', 'number_of_poll_votes', 'number_of_voted_debates', 'opinion_arguments', 'opinion_questions', 'party', 'percentile', 'political_ideology', 'poll_topics', 'poll_votes', 'president', 'relationship', 'religious_ideology', 'url', 'voted_debates', 'win_ratio', 'won_debates', 'tied_debates', 'political_ideology_id', 'political_ideology_len', 'education_id', 'education_len', 'ethnicity_id', 'ethnicity_len', 'interested_id', 'interested_len', 'gender_id', 'gender_len', 'religious_ideology_id', 'religious_ideology_len'])\n",
      "dict_keys(['url', 'category', 'title', 'comments', 'votes', 'rounds', 'forfeit_label', 'forfeit_side', 'start_date', 'update_date', 'voting_style', 'debate_status', 'number_of_comments', 'number_of_views', 'number_of_rounds', 'number_of_votes', 'participant_1_link', 'participant_1_name', 'participant_1_points', 'participant_1_position', 'participant_1_status', 'participant_2_link', 'participant_2_name', 'participant_2_points', 'participant_2_position', 'participant_2_status'])\n",
      "0 is an even number.\n",
      "False\n",
      "Chunkymilk\n",
      "[{'side': 'Pro', 'text': '\\n  \\r\\nI think 0 is an even number so I think it is neither neutral or odd.'}, {'side': 'Con', 'text': '\\n  \\r\\n0 is not an even number or a odd number it is neutral (not helping or supporting either side) basically zero is neutral, because its not positive or negative. its also in between positive and negative integers.'}]\n",
      "{'user_name': '2-D', 'time': '4 years ago', 'votes_map': {'Chunkymilk': {'Agreed with before the debate': False, 'Agreed with after the debate': False, 'Who had better conduct': False, 'Had better spelling and grammar': False, 'Made more convincing arguments': True, 'Used the most reliable sources': True, 'Total points awarded': '5'}, 'pensfan': {'Agreed with before the debate': False, 'Agreed with after the debate': False, 'Who had better conduct': False, 'Had better spelling and grammar': False, 'Made more convincing arguments': False, 'Used the most reliable sources': False, 'Total points awarded': '0'}, 'Tied': {'Agreed with before the debate': True, 'Agreed with after the debate': True, 'Who had better conduct': True, 'Had better spelling and grammar': True, 'Made more convincing arguments': False, 'Used the most reliable sources': False}}}\n"
     ]
    }
   ],
   "source": [
    "number_of_users=len(users)\n",
    "# number_of_users\n",
    "print(users[\"ahuggies30\"].keys())\n",
    "# print(len(users[\"ahuggies30\"].keys()))\n",
    "\n",
    "print(debates[\"0-is-an-even-number./1/\"].keys())\n",
    "\n",
    "print(debates[\"0-is-an-even-number./1/\"]['title'])\n",
    "print(debates[\"0-is-an-even-number./1/\"]['forfeit_label'])\n",
    "print(debates[\"0-is-an-even-number./1/\"]['participant_1_name'])\n",
    "print(debates[\"0-is-an-even-number./1/\"]['rounds'][0])\n",
    "print(debates[\"0-is-an-even-number./1/\"]['votes'][0])\n",
    "# print(users[\"Chunkymilk\"])\n",
    "\n",
    "# categories = [\"birthday\",\"education\",\"ethnicity\",\"gender\",\"looking\",\"number_of_friends\",\"party\",\"political_ideology\",\"president\",\"relationship\",\"religious_ideology\"]\n",
    "# print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in debates.items():\n",
    "    if value['participant_1_position'] != \"Pro\":\n",
    "        print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  2893\n",
      "Votes 10441\n"
     ]
    }
   ],
   "source": [
    "# get number of involved debates:\n",
    "count_list  = []\n",
    "all_count = 0\n",
    "all_votes = 0\n",
    "involved_debates = []\n",
    "\n",
    "for issue in BIG_ISSUES:\n",
    "    count_list.append(0)\n",
    "    \n",
    "for key,value in debates.items():\n",
    "    title = value[\"title\"]\n",
    "\n",
    "    pnames = []\n",
    "    \n",
    "    pnames.append(value['participant_1_name'])\n",
    "    pnames.append(value['participant_2_name'])\n",
    "    \n",
    "    names = []\n",
    "    for vote in value[\"votes\"]:\n",
    "        if vote[\"votes_map\"].get(value['participant_1_name'], -1) == -1:\n",
    "            continue\n",
    "        if vote[\"votes_map\"].get(value['participant_2_name'], -1) == -1:\n",
    "            continue\n",
    "        if vote[\"votes_map\"].get(\"Tied\", -1) == -1:\n",
    "            names.append(vote[\"user_name\"])\n",
    "        elif vote[\"votes_map\"][\"Tied\"]['Made more convincing arguments'] == False:\n",
    "            names.append(vote[\"user_name\"])\n",
    "            \n",
    "    if len(names) == 0:\n",
    "        continue\n",
    "        \n",
    "    puser_available = 1\n",
    "    \n",
    "    for name in pnames:\n",
    "        if users.get(name, -1) == -1:\n",
    "            puser_available = 0\n",
    "    \n",
    "    user_available = 0\n",
    "    \n",
    "    for name in names:\n",
    "        if users.get(name, -1) != -1:\n",
    "            user_available += 1\n",
    "    \n",
    "    if not user_available or not puser_available:\n",
    "        continue\n",
    "    \n",
    "    for i, issue in enumerate(BIG_ISSUES):\n",
    "        if issue in title or issue.lower() in title:\n",
    "            count_list[i] += 1\n",
    "            all_count += 1\n",
    "            all_votes += user_available\n",
    "            \n",
    "            involved_debates.append(value)\n",
    "    \n",
    "print(\"All: \", len(involved_debates))\n",
    "print(\"Votes\", all_votes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(list_of_list):\n",
    "    flat_list = []\n",
    "    for i in list_of_list:\n",
    "        flat_list+=i\n",
    "        \n",
    "    return flat_list\n",
    "\n",
    "def compute_cosine(a, b):\n",
    "\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_ISSUES = ['Abortion', 'Affirmative Action', 'Animal Rights', 'Barack Obama', 'Border Fence',\n",
    "              'Capitalism', 'Civil Unions', 'Death Penalty', 'Drug Legalization', 'Electoral College',\n",
    "              'Environmental Protection', 'Estate Tax', 'European Union', 'Euthanasia', 'Federal Reserve',\n",
    "              'Flat Tax', 'Free Trade', 'Gay Marriage', 'Global Warming Exists', 'Globalization', 'Gold Standard',\n",
    "              'Gun Rights', 'Homeschooling', 'Internet Censorship', 'Iran-Iraq War', 'Labor Union',\n",
    "              'Legalized Prostitution', 'Medicaid & Medicare', 'Medical Marijuana', 'Military Intervention',\n",
    "              'Minimum Wage', 'National Health Care', 'National Retail Sales Tax', 'Occupy Movement', 'Progressive Tax',\n",
    "              'Racial Profiling', 'Redistribution', 'Smoking Ban', 'Social Programs', 'Social Security', 'Socialism',\n",
    "              'Stimulus Spending', 'Term Limits', 'Torture', 'United Nations', 'War in Afghanistan', 'War on Terror', 'Welfare']\n",
    "\n",
    "USEFUL_CATS = ['political_ideology', 'education', 'ethnicity', 'interested', 'gender' , 'religious_ideology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_feature_generator(user_data, name, att_emb_dic):\n",
    "    user = user_data[name]\n",
    "    \n",
    "    # background features\n",
    "    cat_one_hot = []\n",
    "    cat_emb = []\n",
    "    \n",
    "    for cat in USEFUL_CATS:\n",
    "        cat_emb.extend(att_emb_dic[cat + \":\" +user[cat]])\n",
    "\n",
    "        cat_id = user[cat+\"_id\"]\n",
    "        cat_num = user[cat+\"_len\"]\n",
    "        one_hot = [0.0 for i in range(cat_num)]\n",
    "        one_hot[cat_id] = 1.0\n",
    "        cat_one_hot.extend(one_hot)\n",
    "        \n",
    "    # opinions\n",
    "    options = [\"Pro\", \"Con\", \"Und\", \"N/O\", \"N/S\"]\n",
    "    op_one_hot = []\n",
    "    op_emb = []\n",
    "    \n",
    "    for issue in BIG_ISSUES:\n",
    "        op_emb.extend(att_emb_dic[issue+\"-\"+user[\"big_issues_dict\"][issue]])\n",
    "        \n",
    "        op_id = options.index(user[\"big_issues_dict\"][issue])\n",
    "        one_hot = [0.0 for i in range(len(options))]\n",
    "        one_hot[op_id] = 1.0\n",
    "        op_one_hot.extend(one_hot)\n",
    "        \n",
    "    return [cat_one_hot, cat_emb, op_one_hot, op_emb]\n",
    "\n",
    "\n",
    "def argument_topic_feature(sent_list, title):\n",
    "\n",
    "    for sent in sent_list:\n",
    "        if sent[-1] in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "            sent += \".\"\n",
    "\n",
    "    sep = \" \"\n",
    "    text = sep.join(sent_list)\n",
    "    \n",
    "    # relevancy\n",
    "    relevancy = 0.0\n",
    "    rel_count = 0\n",
    "    topic_words = word_tokenize(title)\n",
    "    \n",
    "    for word in topic_words:\n",
    "        if word not in stopWords:\n",
    "            rel_count += 1\n",
    "            relevancy += text.count(word)\n",
    "    \n",
    "    relevancy /= rel_count\n",
    "    \n",
    "    # Consistency\n",
    "    exist_pos = 0\n",
    "    exist_neg = 0\n",
    "    \n",
    "    for sent in sent_list:\n",
    "        rel = 0\n",
    "        for word in topic_words:\n",
    "            if word not in stopWords:\n",
    "                rel = 1\n",
    "        \n",
    "        if rel:\n",
    "            sent_blob = TextBlob(text)\n",
    "            average_polar = sent_blob.sentiment.polarity\n",
    "            if average_polar > 0:\n",
    "                exist_pos = 1\n",
    "            elif average_polar < 0:\n",
    "                exist_neg = 1\n",
    "    \n",
    "    if exist_pos != exist_neg:\n",
    "        consistency = 1\n",
    "    else:\n",
    "        consistency = 0\n",
    "        \n",
    "    return [relevancy, consistency]\n",
    "    \n",
    "    \n",
    "def linguistic_feature_generator(sent_list, feature_set=[\"len\",\"sub-polar\",\"arglex\",\"referring_op\", \"links\"]):\n",
    "    \n",
    "    linguistic_vec = []\n",
    "    for sent in sent_list:\n",
    "        if sent[-1] in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "            sent += \".\"\n",
    "\n",
    "    sep = \" \"\n",
    "    text = sep.join(sent_list)\n",
    "\n",
    "    avg_length = 0\n",
    "    avg_sub = 0\n",
    "    avg_polar = 0\n",
    "    count = len(sent_list)\n",
    "\n",
    "    # failed situtaion\n",
    "    if count == 0:\n",
    "        return [0.0 for i in range(10)]\n",
    "\n",
    "    # sentiment\n",
    "    sent_blob = TextBlob(text)\n",
    "    average_sub = sent_blob.sentiment.subjectivity\n",
    "    average_polar = sent_blob.sentiment.polarity\n",
    "\n",
    "    # len\n",
    "    for sent in sent_list:\n",
    "        avg_length += len(word_tokenize(sent))\n",
    "\n",
    "    avg_length = avg_length/count\n",
    "\n",
    "    # arg\n",
    "    lexicon_score = arglex.analyse(text)\n",
    "    # in the original arglex, we have\n",
    "    # ['0-Assessments', '1-Authority', '2-Causation', '3-Conditionals', '4-Contrast', '5-Difficulty', '6-Doubt', '7-Emphasis',\\\n",
    "    #     '8-Generalization', '9-Inconsistency', '10-Inyourshoes', '11-Necessity', '12-Possibility', '13-Priority', '14-Rhetoricalquestion',\\\n",
    "    #    '15-Structure', '16-Wants']\n",
    "    # Here we only care about Authority, Conditionals, Contrast, Difficulty, Necessity\n",
    "    lex_vec = [lexicon_score[1], lexicon_score[3], lexicon_score[4], lexicon_score[5], lexicon_score[11]]\n",
    "    \n",
    "    # referring opponent\n",
    "    ref_op = 0.0\n",
    "    if \"opponent\" in text or \"debater\" in text:\n",
    "        ref_op = 1.0\n",
    "    \n",
    "    # using links:\n",
    "    use_links = 0\n",
    "    if \"www.\" in text or \"http\" in text:\n",
    "        use_links = 1\n",
    "    \n",
    "    if \"len\" in feature_set:\n",
    "        linguistic_vec.append(avg_length)\n",
    "    if \"sub-polar\" in feature_set:\n",
    "        linguistic_vec.append(average_sub)\n",
    "        linguistic_vec.append(average_polar)\n",
    "    if \"arglex\" in feature_set:\n",
    "        linguistic_vec.extend(lex_vec)\n",
    "    if \"referring_op\" in feature_set:\n",
    "        linguistic_vec.extend([ref_op])\n",
    "    if \"links\" in feature_set:\n",
    "        linguistic_vec.extend([use_links])\n",
    "        \n",
    "    return linguistic_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_debate_data(debates, users, useful_cats, max_arg_sent, issue_emb_dic , att_emb_dic):\n",
    "    dataset = []\n",
    "    \n",
    "    for debate in tqdm(debates):\n",
    "        \n",
    "        debater1 = users[debate['participant_1_name']]\n",
    "        debater2 = users[debate['participant_2_name']]\n",
    "        title = debate[\"title\"]\n",
    "        \n",
    "        # issue_emb\n",
    "        this_issue = \"\"\n",
    "        for i, issue in enumerate(BIG_ISSUES):\n",
    "            if issue in title or issue.lower() in title:\n",
    "                this_issue = issue\n",
    "                break\n",
    "                \n",
    "        issue_emb = issue_emb_dic[this_issue]\n",
    "        \n",
    "        # [cat_one_hot, cat_emb, op_one_hot, op_emb]\n",
    "        uf1 =  user_feature_generator(users, debate['participant_1_name'], att_emb_dic)\n",
    "        uf2 =  user_feature_generator(users, debate['participant_2_name'], att_emb_dic)\n",
    "        \n",
    "        valid_votes = []\n",
    "        for vote in debate[\"votes\"]:\n",
    "            if users.get(vote['user_name'], -1) == -1:\n",
    "                continue\n",
    "            \n",
    "            if vote[\"votes_map\"].get(debate['participant_1_name'], -1) == -1:\n",
    "                continue\n",
    "            if vote[\"votes_map\"].get(debate['participant_2_name'], -1) == -1:\n",
    "                continue\n",
    "            \n",
    "            if vote[\"votes_map\"].get(\"Tied\", -1) == -1:\n",
    "                valid_votes.append(vote)\n",
    "            elif vote[\"votes_map\"][\"Tied\"]['Made more convincing arguments'] == False:\n",
    "                valid_votes.append(vote)\n",
    "                \n",
    "        if len(valid_votes) == 0:\n",
    "            continue\n",
    "                \n",
    "        debater1_args = []\n",
    "        debater2_args = []\n",
    "        \n",
    "                \n",
    "        for debate_round in debate[\"rounds\"]:\n",
    "            for argument in debate_round:\n",
    "                temp_text = argument[\"text\"].replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\").lstrip().rstrip()\n",
    "                temp_sents = sent_tokenize(temp_text)\n",
    "                \n",
    "                if len(temp_sents) > 3:\n",
    "                    temp_sents = temp_sents[-4:]\n",
    "                \n",
    "                if argument[\"side\"] == \"Pro\":\n",
    "                    debater1_args.extend(temp_sents)\n",
    "                if argument[\"side\"] == \"Con\":\n",
    "                    debater2_args.extend(temp_sents)\n",
    "                    \n",
    "        \n",
    "        # create the dataset\n",
    "        \n",
    "        lf1 = linguistic_feature_generator(debater1_args)\n",
    "        lf2 = linguistic_feature_generator(debater2_args)\n",
    "        \n",
    "        atf1 = argument_topic_feature(debater1_args, title)\n",
    "        atf2 = argument_topic_feature(debater2_args, title)\n",
    "        \n",
    "        debater1_score = 0\n",
    "        debater2_score = 0\n",
    "        \n",
    "        voters_ufs = []\n",
    "        \n",
    "        for vote in valid_votes:\n",
    "            voters_ufs.append(user_feature_generator(users, vote['user_name'], att_emb_dic))\n",
    "            \n",
    "            if vote[\"votes_map\"][debate['participant_1_name']].get('Made more convincing arguments', -1) != -1:\n",
    "                if vote[\"votes_map\"][debate['participant_1_name']]['Made more convincing arguments'] == True:\n",
    "                    debater1_score += 1\n",
    "                else:\n",
    "                    debater2_score += 1\n",
    "            else:\n",
    "                debater2_score += 1\n",
    "        \n",
    "        # one-hot b sim, o sim; emb b sim osim\n",
    "        d1_sims = [0,0,0,0]\n",
    "        d2_sims = [0,0,0,0]\n",
    "        \n",
    "        for uf in voters_ufs:\n",
    "            # [cat_one_hot, cat_emb, op_one_hot, op_emb]\n",
    "            for i in range(4):\n",
    "                d1_sims[i] += compute_cosine(uf1[i], uf[i])\n",
    "                d2_sims[i] += compute_cosine(uf2[i], uf[i])\n",
    "                \n",
    "        for i in range(4):\n",
    "            d1_sims[i] /= len(voters_ufs)\n",
    "            d2_sims[i] /= len(voters_ufs)\n",
    "            \n",
    "        \n",
    "        if debater1_score > debater2_score:\n",
    "            label1 = 1\n",
    "            label2 = 0\n",
    "        elif debater1_score < debater2_score:\n",
    "            label1 = 0\n",
    "            label2 = 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        # participant 1 data\n",
    "        temp  = {}\n",
    "        temp[\"title\"] = debate[\"title\"]\n",
    "        temp[\"debate_key\"] = key\n",
    "        temp[\"issue\"] = this_issue\n",
    "        temp[\"issue_emb\"] = issue_emb\n",
    "        \n",
    "        temp[\"user_name\"] = debate['participant_1_name']\n",
    "        \n",
    "        temp[\"cat_one_hot\"] = uf1[0]\n",
    "        temp[\"cat_emb\"] = uf1[1]\n",
    "        temp[\"op_one_hot\"] = uf1[2]\n",
    "        temp[\"op_emb\"] = uf1[3]\n",
    "        temp[\"sims\"] = d1_sims \n",
    "        temp[\"sim_ref\"] = [\"cat_one_hot\", \"cat_emb\", \"op_one_hot\", \"op_emb\"]\n",
    "        \n",
    "        temp[\"ling_features\"] = lf1\n",
    "        temp[\"topic_features\"] = atf1\n",
    "        temp[\"args\"] = debater1_args\n",
    "        temp[\"label\"] = label1\n",
    "        \n",
    "        dataset.append(temp)\n",
    "        \n",
    "        # participant 2 data\n",
    "        temp  = {}\n",
    "        temp[\"title\"] = debate[\"title\"]\n",
    "        temp[\"debate_key\"] = key\n",
    "        temp[\"issue\"] = this_issue\n",
    "        temp[\"issue_emb\"] = issue_emb\n",
    "        \n",
    "        temp[\"user_name\"] = debate['participant_2_name']\n",
    "        \n",
    "        temp[\"cat_one_hot\"] = uf2[0]\n",
    "        temp[\"cat_emb\"] = uf2[1]\n",
    "        temp[\"op_one_hot\"] = uf2[2]\n",
    "        temp[\"op_emb\"] = uf2[3]\n",
    "        temp[\"sims\"] = d2_sims \n",
    "        temp[\"sim_ref\"] = [\"cat_one_hot\", \"cat_emb\", \"op_one_hot\", \"op_emb\"]\n",
    "        \n",
    "        temp[\"ling_features\"] = lf2\n",
    "        temp[\"topic_features\"] = atf2\n",
    "        temp[\"args\"] = debater2_args\n",
    "        temp[\"label\"] = label2\n",
    "    \n",
    "        dataset.append(temp)\n",
    "        \n",
    "    print(\"Number of the data collected\", len(dataset))\n",
    "\n",
    "    return dataset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4b0f583064b0fa98f67f4b8915313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2893.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of the data collected 5496\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "# print(involved_debates[0])\n",
    "dataset = prepare_debate_data(involved_debates, user_collection, USEFUL_CATS, -1, issue_emb_dic , att_emb_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.845705967976711\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "num_sent = []\n",
    "\n",
    "for data in dataset:\n",
    "    num_sent.append(len(data[\"args\"]))\n",
    "    \n",
    "print(np.mean(num_sent))\n",
    "print(np.max(num_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(users[\"ahuggies30\"][\"big_issues_dict\"].values()))\n",
    "# print(len(users[\"ahuggies30\"][\"big_issues_dict\"].keys()))\n",
    "with open(\"persuasion.json\",\"w\",encoding=\"UTF-8\") as f:\n",
    "    json.dump(dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'debate_key', 'issue', 'issue_emb', 'user_name', 'cat_one_hot', 'cat_emb', 'op_one_hot', 'op_emb', 'sims', 'sim_ref', 'ling_features', 'topic_features', 'args', 'label'])\n",
      "119\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].keys())\n",
    "\n",
    "print(len(dataset[0][\"cat_one_hot\"]))\n",
    "print(len(dataset[0][\"op_one_hot\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "split = {}\n",
    "split[\"train\"] = []\n",
    "split[\"dev\"] = []\n",
    "split[\"test\"] = []\n",
    "\n",
    "\n",
    "mask = list(range(len(dataset)))\n",
    "random.Random(2021).shuffle(mask)\n",
    "\n",
    "for select in mask[:int(0.7*len(mask))]:\n",
    "    split[\"train\"].append(dataset[select])\n",
    "    \n",
    "for select in mask[int(0.7*len(mask)):int(0.85*len(mask))]:\n",
    "    split[\"dev\"].append(dataset[select])\n",
    "    \n",
    "for select in mask[int(0.85*len(mask)):]:\n",
    "    split[\"test\"].append(dataset[select])\n",
    "    \n",
    "with open(\"split.json\",\"w\",encoding=\"UTF-8\") as f:\n",
    "    json.dump(split,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49818181818181817 0.24909090909090909 0.5 0.3325242718446602\n"
     ]
    }
   ],
   "source": [
    "# baselines - majority\n",
    "label = []\n",
    "pred = []\n",
    "\n",
    "for case in split[\"test\"]:\n",
    "    if case[\"label\"] == 1:\n",
    "        label.append(1)\n",
    "        pred.append(1)\n",
    "    if case[\"label\"] == 0:\n",
    "        label.append(0)\n",
    "        pred.append(1)\n",
    "        \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "acc = accuracy_score(label, pred)\n",
    "a = precision_score(label, pred, average='macro')\n",
    "b = recall_score(label, pred, average='macro')\n",
    "c = f1_score(label, pred, average='macro')\n",
    "print(acc, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_svm_data(dataset, fsets=[\"user\", \"ling\", \"topic\"], uset = \"emb\"): # uset = \"one_hot\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # ['title', 'debate_key', 'issue', 'issue_emb', 'user_name', \n",
    "    # 'cat_one_hot', 'cat_emb', 'op_one_hot', 'op_emb', 'sims', 'sim_ref', 'ling_features', 'topic_features', 'args', 'label']\n",
    "    \n",
    "    for datapiece in dataset:\n",
    "        temp = []\n",
    "        \n",
    "        if \"user\" in fsets:\n",
    "            # [\"cat_one_hot\", \"cat_emb\", \"op_one_hot\", \"op_emb\"]\n",
    "            for key in [\"cat_\"+uset, \"op_\"+uset]:\n",
    "                temp.extend(datapiece[key])\n",
    "            \n",
    "            if uset == \"emb\":\n",
    "                temp.append(datapiece[\"sims\"][1])\n",
    "                temp.append(datapiece[\"sims\"][2])\n",
    "                \n",
    "        if \"ling\" in fsets:\n",
    "            temp.extend(datapiece[\"ling_features\"])\n",
    "            \n",
    "        if \"topic\" in fsets:\n",
    "            temp.extend(datapiece[\"topic_features\"])\n",
    "            \n",
    "        X.append(temp)\n",
    "        y.append(datapiece[\"label\"])\n",
    "        \n",
    "    return X, y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6691542288557214 0.6690468634295991 0.6690208837572247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Razer\\.conda\\envs\\wsc_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# svm with rbf\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_X, train_y = prepare_svm_data(split[\"train\"], fsets=[\"topic\", \"user\", \"ling\"], uset = \"one_hot\")\n",
    "test_X, test_y = prepare_svm_data(split[\"test\"], fsets=[\"topic\", \"user\", \"ling\"], uset = \"one_hot\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "pred_y = clf.predict(test_X)\n",
    "\n",
    "a = precision_score(test_y, pred_y, average='macro')\n",
    "b = recall_score(test_y, pred_y, average='macro')\n",
    "c = f1_score(test_y, pred_y, average='macro')\n",
    "print(a, b, c)\n",
    "# Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('svc', SVC(gamma='auto'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_debate_text(user, all_debates):\n",
    "    name = user[\"name\"]\n",
    "    debates = []\n",
    "\n",
    "    debates_names = user[\"all_debates\"]\n",
    "    for debate_name in debates_names:\n",
    "        if all_debates.get(debate_name):\n",
    "            this_debate = all_debates[debate_name]\n",
    "            position = \"\"\n",
    "            if this_debate[\"participant_1_name\"] == name:\n",
    "                position = this_debate[\"participant_1_position\"]\n",
    "            else:\n",
    "                position = this_debate[\"participant_2_position\"]\n",
    "\n",
    "            for round in this_debate[\"rounds\"]:\n",
    "                for argument in round:\n",
    "                    if argument[\"side\"] == position:\n",
    "                        text = argument[\"text\"].replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\").lstrip().rstrip()\n",
    "                        text += \" \"\n",
    "                        debates.append(text)\n",
    "\n",
    "    return debates\n",
    "\n",
    "def get_opinion_text(user):\n",
    "    opinions = []\n",
    "    arguments = user[\"opinion_arguments\"]\n",
    "    \n",
    "    for argument in arguments:\n",
    "        text = argument['opinion text'].replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\").lstrip().rstrip()\n",
    "        text += \" \"\n",
    "        opinions.append(text)\n",
    "        \n",
    "    return opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "from spacy.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm(dataset, vocab, wvmodel):\n",
    "    all_texts = []\n",
    "    all_titles = []\n",
    "    all_labels = []\n",
    "                 \n",
    "    for piece in dataset:\n",
    "        all_text = tokenizer(\" \".join(piece['args']))\n",
    "        all_texts.append(all_text)\n",
    "        title = piece[\"title\"]\n",
    "        all_titles.append(tokenizer(title))\n",
    "                 \n",
    "        all_labels.append(piece[\"label\"])\n",
    "                 \n",
    "    arg_features = torch.tensor(pad_samples(encode_samples(all_texts, vocab)))\n",
    "    title_features = torch.tensor(pad_samples(encode_samples(all_titles, vocab)))\n",
    "    labels = torch.tensor([score for score in all_labels])\n",
    "                 \n",
    "                 \n",
    "    return arg_features, title_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        \n",
    "        super(ArgLSTM, self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        \n",
    "        self.hidden1 = nn.Linear(num_hiddens * 8, num_hiddens * 4)\n",
    "        self.hidden2 = nn.Linear(num_hiddens * 4, num_hiddens * 2)\n",
    "        self.hidden3 = nn.Linear(num_hiddens * 2, num_hiddens * 2)\n",
    "        \n",
    "        self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        embeddings1 = self.embedding(inputs1)\n",
    "        embeddings2 = self.embedding(inputs2)\n",
    "        \n",
    "        states1, hidden1 = self.encoder(embeddings1.permute([1, 0, 2]))\n",
    "        states2, hidden2 = self.encoder(embeddings2.permute([1, 0, 2]))\n",
    "        \n",
    "        encoding = torch.cat([states1[0], states1[-1], states2[0], states2[-1]], dim=1)\n",
    "        encoding = self.hidden3(self.hidden2(self.hidden1(encoding)))\n",
    "        \n",
    "        outputs = self.decoder(encoding)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = []\n",
    "\n",
    "for temp in dataset:\n",
    "    all_args.extend(temp[\"args\"])\n",
    "\n",
    "vocab = set(Vocab(strings=all_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('C:\\\\Users\\Razer\\Desktop\\debate_args\\glove.txt')\n",
    "tmp_file = get_tmpfile(\"C:\\\\Users\\Razer\\Desktop\\debate_args\\word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "wvmodel = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "embed_size = 100\n",
    "num_hiddens = 200\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.08\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "word_to_idx  = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'\n",
    "vocab_size = len(vocab)\n",
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "\n",
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "net =  ArgLSTM(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "tr_args, tr_titles, tr_labels = prepare_lstm(split[\"train\"], vocab, wvmodel)\n",
    "tst_args, tst_titles, tst_labels = prepare_lstm(split[\"test\"], vocab, wvmodel)\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(tr_args, tr_titles, tr_labels)\n",
    "test_set = torch.utils.data.TensorDataset(tst_args, tst_titles, tst_labels)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.2509090909090909 0.5 0.3341404358353511\n",
      "0.24909090909090909 0.5 0.3325242718446602\n",
      "0.24909090909090909 0.5 0.3325242718446602\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    test_labels, test_preds = [], []\n",
    "    n, m = 0, 0\n",
    "    for args, titles, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        args = Variable(args.cuda())\n",
    "        titles = Variable(titles.cuda())\n",
    "        label = Variable(label.cuda())\n",
    "        \n",
    "        score = net(args, titles)\n",
    "        \n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for args, titles, label in test_iter:\n",
    "            m += 1\n",
    "            \n",
    "            args = Variable(args.cuda())\n",
    "            titles = Variable(titles.cuda())\n",
    "            tensor_label = Variable(label.cuda())\n",
    "            test_score = net(args, titles)\n",
    "            test_loss = loss_function(test_score, tensor_label)\n",
    "            \n",
    "            out_preds = torch.argmax(test_score.cpu().data, dim=1)\n",
    "            test_preds.extend(out_preds.tolist())\n",
    "            test_labels.extend(label.tolist())\n",
    "            \n",
    "            test_losses += test_loss\n",
    "            \n",
    "    a = precision_score(test_labels, test_preds, average='macro')\n",
    "    b = recall_score(test_labels, test_preds, average='macro')\n",
    "    c = f1_score(test_labels, test_preds, average='macro')\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
